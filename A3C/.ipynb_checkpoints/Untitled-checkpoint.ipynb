{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6b3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import v_wrap, set_init, push_and_pull, record\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import gym\n",
    "import math, os\n",
    "import numpy as np\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "UPDATE_GLOBAL_ITER = 5\n",
    "GAMMA = 0.9\n",
    "MAX_EP = 3000\n",
    "MAX_EP_STEP = 200\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2fd8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # State initialization\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # share in memory\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "775fd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## numpy array to torch tensor\n",
    "def v_wrap(np_array, dtype=np.float32):\n",
    "    if np_array.dtype != dtype:\n",
    "        np_array = np_array.astype(dtype)\n",
    "    return torch.from_numpy(np_array)\n",
    "\n",
    "\n",
    "def set_init(layers):\n",
    "    for layer in layers:\n",
    "        nn.init.normal_(layer.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(layer.bias, 0.)\n",
    "\n",
    "\n",
    "def push_and_pull(opt, lnet, gnet, done, s_, bs, ba, br, gamma):\n",
    "    if done:\n",
    "        v_s_ = 0.               # terminal\n",
    "    else:\n",
    "        v_s_ = lnet.forward(v_wrap(s_[None, :]))[-1].data.numpy()[0, 0]\n",
    "\n",
    "    buffer_v_target = []\n",
    "    for r in br[::-1]:    # reverse buffer r\n",
    "        v_s_ = r + gamma * v_s_\n",
    "        buffer_v_target.append(v_s_)\n",
    "    buffer_v_target.reverse()\n",
    "\n",
    "    loss = lnet.loss_func(\n",
    "        v_wrap(np.vstack(bs)),\n",
    "        v_wrap(np.array(ba), dtype=np.int64) if ba[0].dtype == np.int64 else v_wrap(np.vstack(ba)),\n",
    "        v_wrap(np.array(buffer_v_target)[:, None]))\n",
    "\n",
    "    # calculate local gradients and push local parameters to global\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    for lp, gp in zip(lnet.parameters(), gnet.parameters()):\n",
    "        gp._grad = lp.grad\n",
    "    opt.step()\n",
    "\n",
    "    # pull global parameters\n",
    "    lnet.load_state_dict(gnet.state_dict())\n",
    "\n",
    "\n",
    "def record(global_ep, global_ep_r, ep_r, res_queue, name):\n",
    "    with global_ep.get_lock():\n",
    "        global_ep.value += 1\n",
    "    with global_ep_r.get_lock():\n",
    "        if global_ep_r.value == 0.:\n",
    "            global_ep_r.value = ep_r\n",
    "        else:\n",
    "            global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01\n",
    "    res_queue.put(global_ep_r.value)\n",
    "    print(\n",
    "        name,\n",
    "        \"Ep:\", global_ep.value,\n",
    "        \"| Ep_r: %.0f\" % global_ep_r.value,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a9bbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.a1 = nn.Linear(s_dim, 200)\n",
    "        self.mu = nn.Linear(200, a_dim)\n",
    "        self.sigma = nn.Linear(200, a_dim)\n",
    "        self.c1 = nn.Linear(s_dim, 100)\n",
    "        self.v = nn.Linear(100, 1)\n",
    "        set_init([self.a1, self.mu, self.sigma, self.c1, self.v])\n",
    "        self.distribution = torch.distributions.Normal\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = F.relu6(self.a1(x))\n",
    "        mu = 2 * F.tanh(self.mu(a1))\n",
    "        sigma = F.softplus(self.sigma(a1)) + 0.001      # avoid 0\n",
    "        c1 = F.relu6(self.c1(x))\n",
    "        values = self.v(c1)\n",
    "        return mu, sigma, values\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        self.training = False\n",
    "        mu, sigma, _ = self.forward(s)\n",
    "        m = self.distribution(mu.view(1, ).detach(), sigma.view(1, ).detach())\n",
    "        return m.sample().numpy()\n",
    "\n",
    "    def loss_func(self, s, a, v_t):\n",
    "        self.train()\n",
    "        mu, sigma, values = self.forward(s)\n",
    "        td = v_t - values\n",
    "        c_loss = td.pow(2)\n",
    "\n",
    "        m = self.distribution(mu, sigma)\n",
    "        log_prob = m.log_prob(a)\n",
    "        entropy = 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(m.scale)  # exploration\n",
    "        exp_v = log_prob * td.detach() + 0.005 * entropy\n",
    "        a_loss = -exp_v\n",
    "        total_loss = (a_loss + c_loss).mean()\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104066c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57fa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
